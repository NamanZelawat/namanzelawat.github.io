<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <link rel="stylesheet" href="https://latex.vercel.app/style.css" />
    <title>Naman Zelawat</title>
    <style>
      html {
        scroll-behavior: smooth;
      }
      h1 {
        padding: 20px;
        margin-bottom: 0;
      }
      p {
        font-size: 1.2rem;
        color: #111;
        padding: 0;
        margin: 0;
      }
      ul {
        font-size: 1.2rem;
        color: #111;
      }
      body {
        max-width: 750px;
      }
      /* Container wrapper */
      .latex-tabs {
        font-family: inherit;
        margin: 2em 0;
      }

      /* Hide native radios */
      .latex-tabs input[type="radio"] {
        display: none;
      }

      /* Tab labels as table row */
      .latex-tabs .tab-labels {
        display: table;
        table-layout: fixed;
        width: 100%;
        margin-bottom: -1px; /* pull up border of content */
      }

      .latex-tabs .tab-labels label {
        display: table-cell;
        text-align: center;
        padding: 0.75em 1em;
        border: 1px solid #000;
        border-bottom: none;
        background: #f5f5f5;
        cursor: pointer;
        user-select: none;
      }

      /* Active tab look */
      .latex-tabs input[type="radio"]:checked + label {
        background: #fff;
        font-weight: bold;
      }

      /* Content panels styled like table */
      .latex-tabs .tab-content {
        display: none;
        border: 1px solid #000;
        padding: 1em;
        background: #fff;
      }

      /* Show panel when its radio is checked */
      #tab1:checked ~ #content1,
      #tab2:checked ~ #content2,
      #tab3:checked ~ #content3 {
        display: block;
      }
      .nav-bar {
        text-align: center;
        font-family: "Computer Modern", serif; /* Optional: for LaTeX aesthetic */
        font-size: 1.1em;
        margin-top: 0;
      }

      .nav-bar a {
        text-decoration: none;
        color: inherit; /* Keeps it neutral, blends with text */
        padding: 0 0.3em;
        cursor: pointer;
        transition: opacity 0.2s ease;
      }

      .nav-bar a:hover {
        opacity: 0.7; /* Just enough to hint interactivity */
      }

      .container {
        display: flex;
        flex-direction: column;
        /* align-items: center; */
        /* max-width: 1200px; */
        margin: 0 auto;
        padding: 2rem;
        /* background: #fff; */
        box-shadow: 0 4px 6px rgba(0, 0, 0, 0.05);
        /* border-radius: 8px; */
      }
      .content-wrapper {
        display: flex;
        flex-direction: column;
        gap: 2rem;
        width: 100%;
      }
      @media (min-width: 768px) {
        .content-wrapper {
          flex-direction: row;
          justify-content: space-between;
          /* align-items: center; */
        }
      }
      .text-section {
        flex: 1;
        padding-right: 2rem;
      }
      .image-section {
        flex: 1;
        display: flex;
        justify-content: center;
        align-items: center;
      }
      .image-section img {
        max-width: 400px;
        height: 400px;
        width: 100%;
        object-fit: cover;
        border-radius: 8px;
        box-shadow: 0 2px 4px rgba(0, 0, 0, 0.1);
      }
      h1 {
        font-size: 2.2rem;
        line-height: 1.2;
        margin-top: 0;
        color: #1a1a1a;
      }
      p {
        font-size: 1.1rem;
        line-height: 1.6;
      }
      .link-button {
        display: inline-block;
        padding: 0.75rem 1.5rem;
        background: #1a1a1a;
        color: #fff;
        text-decoration: none;
        border-radius: 5px;
        transition: background-color 0.3s ease;
      }
      .link-button:hover {
        background: #555;
      }
    </style>
  </head>
  <body>
    <h1>Projects</h1>

    <ul>
      <li>
        <strong>ViT paper implementation using PyTorch</strong>
      </li>
      <ul>
        <li>
          I implemented the Vision Transformer (ViT) architecture from scratch,
          visualizing the embedding creation process and training the model on
          the Food-101 dataset for image classification. To further enhance
          performance, I incorporated transfer learning by leveraging pretrained
          weights from PyTorch models, which significantly improved the overall
          accuracy.
        </li>
        <img
          src="./assets/transformer.png"
          alt="Image patches"
          ,
          width="300"
          style="display: block; margin-left: auto; margin-right: auto"
        />
        <li>
          <strong>Domain</strong>: Paper implementation, Computer Vision, Deep
          Learning, PyTorch.
        </li>
        <li>
          <a
            href="https://github.com/NamanZelawat/Gym/blob/master/pytorch/paper_replication.ipynb"
            >Code</a
          >
        </li>
      </ul>

      <li>
        <strong>Transfer learning using PyTorch</strong>
      </li>
      <ul>
        <li>
          I utilized pretrained EfficientNet-B0 weights to classify specific
          subcategories of the Food-101 dataset, testing the modelâ€™s
          generalization on real-world data beyond the test set. To streamline
          the workflow, I developed custom wrappers that simplified data
          preparation, training, evaluation, and visualization, making the
          experimentation process more efficient and reproducible.
        </li>
        <img
          src="./assets/transfer.png"
          alt="Image patches"
          ,
          width="300"
          style="display: block; margin-left: auto; margin-right: auto"
        />
        <li>
          <strong>Domain</strong>: Transfer learning, Computer Vision, Deep
          Learning, PyTorch.
        </li>
        <li>
          <a
            href="https://github.com/NamanZelawat/Gym/blob/master/pytorch/transfer_learning.ipynb"
            >Code</a
          >
        </li>
      </ul>

      <li>
        <strong>Fashion MNIST solution in PyTorch</strong>
      </li>
      <ul>
        <li>
          I worked on predictions using the FashionMNIST dataset, modularizing
          the code to simplify training and testing across three different
          models. The implementation was designed to be device-agnostic,
          ensuring compatibility across CPUs and GPUs, and included detailed
          time analysis for both training and testing phases.
        </li>
        <img
          src="./assets/fashion.png"
          alt="Image patches"
          ,
          width="300"
          style="display: block; margin-left: auto; margin-right: auto"
        />
        <li><strong>Domain</strong>: Deep Learning, PyTorch.</li>
        <li>
          <a
            href="https://github.com/NamanZelawat/Gym/blob/master/pytorch/CNN_computer_vision.ipynb"
            >Code</a
          >
        </li>
      </ul>
    </ul>
  </body>
</html>

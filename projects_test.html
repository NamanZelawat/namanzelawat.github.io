<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <link rel="stylesheet" href="https://latex.vercel.app/style.css" />
    <title>Naman Zelawat</title>
    <style>
      /* Main body styling for context */
      body {
        display: flex;
        flex-direction: column;
        /* justify-content: center; */
        align-items: center;
        min-height: 100vh;
        background-color: #f0f2f5;
        font-family: "Roboto", sans-serif;
        padding: 2rem;
        box-sizing: border-box;
        max-width: 800px;
      }

      /* The main container for the research paper card */
      .latex-card {
        background: #ffffff;
        border: 1px solid #d1d1d1;
        box-shadow: 0 4px 12px rgba(0, 0, 0, 0.08);
        /* max-width: 600px; */
        width: 100%;
        padding: 2.5rem 3rem;
        line-height: 1.6;
        border-radius: 4px;
      }

      /* Title of the project/paper */
      .paper-title {
        font-size: 1.8rem;
        font-weight: bold;
        text-align: center;
        margin: 0 0 0.5rem 0;
        line-height: 1.3;
      }

      /* Author information */
      .authors {
        text-align: center;
        font-size: 1rem;
        font-style: italic;
        color: #555;
        margin-bottom: 2rem;
      }

      /* Abstract section styling */
      .abstract {
        margin-bottom: 2rem;
        text-align: justify;
        border-left: 2px solid #ccc;
        padding-left: 1rem;
        font-size: 0.9rem;
        color: #444;
      }

      .abstract h3 {
        font-size: 1.1rem;
        font-weight: bold;
        margin: 0 0 0.5rem 0;
      }

      /* Section styling */
      .section h2 {
        font-size: 1.3rem;
        font-weight: bold;
        border-bottom: 1px solid #333;
        padding-bottom: 0.3rem;
        margin-top: 1rem;
        margin-bottom: 1rem;
      }

      .section p {
        font-size: 1rem;
        text-align: justify;
        margin-bottom: 1.5rem;
      }

      /* Figure styling for the image */
      .figure {
        margin: 2rem 0;
        text-align: center;
      }

      .figure img {
        display: block;
        margin-left: auto;
        margin-right: auto;
        max-width: 100%;
        height: auto;
        border: 1px solid #ddd;
        border-radius: 2px;
        margin-bottom: 0.5rem;
      }

      /* Caption for the figure */
      .caption {
        font-size: 0.9rem;
        font-style: italic;
        color: #666;
      }

      .caption b {
        font-weight: bold;
      }

      /* Responsive adjustments */
      @media (max-width: 600px) {
        body {
          padding: 1rem;
        }
        .latex-card {
          padding: 2rem 1.5rem;
        }
        .paper-title {
          font-size: 1.5rem;
        }
      }
    </style>
  </head>
  <body>
    <!-- LaTeX-Style Project Card -->
    <div class="latex-card">
      <h1 class="paper-title">Projects</h1>

      <div class="section">
        <h2>ViT paper implementation using PyTorch</h2>
        <p>
          The core of the project is a custom-built rendering engine developed
          in C++ and CUDA. The engine interfaces with a Python-based machine
          learning module responsible for training and querying the NeRF model.
          We utilized a dataset of synthetic scenes to train the network to
          predict volumetric density and color, which is then integrated into
          the final render.
        </p>

        <!-- Image Figure -->
        <div class="figure">
          <img
            src="https://placehold.co/500x280/eeeeee/333333?text=Project+Diagram"
            alt="Diagram of the project architecture"
          />
          <p class="caption">
            <b>Fig. 1:</b> High-level overview of the hybrid rendering pipeline,
            showing the interaction between the ray-tracing kernel and the NeRF
            inference module.
          </p>
        </div>
      </div>
    </div>
    <!-- <h1>Projects</h1>

    <ul>
      <li>
        <strong>ViT paper implementation using PyTorch</strong>
      </li>
      <ul>
        <li>
          I implemented the Vision Transformer (ViT) architecture from scratch,
          visualizing the embedding creation process and training the model on
          the Food-101 dataset for image classification. To further enhance
          performance, I incorporated transfer learning by leveraging pretrained
          weights from PyTorch models, which significantly improved the overall
          accuracy.
        </li>
        <img
          src="./assets/transformer.png"
          alt="Image patches"
          ,
          width="300"
          style="display: block; margin-left: auto; margin-right: auto"
        />
        <li>
          <strong>Domain</strong>: Paper implementation, Computer Vision, Deep
          Learning, PyTorch.
        </li>
        <li>
          <a
            href="https://github.com/NamanZelawat/Gym/blob/master/pytorch/paper_replication.ipynb"
            >Code</a
          >
        </li>
      </ul>
    </ul> -->
  </body>
</html>
